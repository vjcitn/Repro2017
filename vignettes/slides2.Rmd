---
title: "Reproducible research concepts and tools, 2017"
author: "Vincent J. Carey, stvjc at channing.harvard.edu"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: ioslides_presentation
runtime: shiny
---

```{r setup,echo=FALSE}
suppressPackageStartupMessages({
library(png)
library(grid)
})
```

## Resource

```{r foocov,echo=FALSE}
im = readPNG("images/workshopFace.png")
grid.raster(im)
```

## Task

```{r footask,echo=FALSE}
im = readPNG("images/NAStask.png")
grid.raster(im)
```

## The three themes underlying reproducibility research

1) providing **code** and **data** and *environment* to **independent parties**
to **diminish risk** of analyses that are **not reproducible**

2) fortifying **criteria of statistical soundness** of analyses (study interpretations) to **control risk** of **non-replicability** of **studies**

3) doing 1) and 2) in ways that are cost-effective

## Y. Benjamini, [NAS workshop](https://errorstatistics.files.wordpress.com/2016/02/conceptualizing-measuring-and-studying-reproducibility.pdf) p. 47

**[R]eproducibility is a property of a study, and replicability is a property of a result that can be proved only by inspecting other results of similar experiments. Therefore, the reproducibility of a result from a single study can be assured, and improving the statistical analysis can enhance its replicability.**

My reading:

- assuring reproducibility requires technique by the investigator
- enhancing replicability requires new approaches to statistical measurement of evidence

## Road map of talk

* Basic terminology
* Reflections on GWAS replication and genetic medicine
* Replication concepts in the NAS workshop
* Reproducible research recommendations of ASA
* Some case studies
* Some github exercises

## A personal view

* Extensibility and transportability **should not** be divorced from reproducibility
    - Reproducer/reader should be able to assess effects of modifications to queries and inferences

* Scalability **cannot** be divorced from reproducbility
    - Can I check a computation that took the author days to create?
    - Does the work support a [stepwise approach to verification](https://arxiv.org/abs/1409.3531)?
        - Results are reproducible in detail for a meaningful subproblem
        - Results are reproducible in detail for a sequence of
meaningful subproblems of increasing difficulty

* **Computable documents** (rmarkdown, jupyter, ...) are essential elements of cost-effective reproducible research

## Basic definitions

1) Analysis = software + data + environment + invocations

2) Reproducible analysis = analysis that can be carried out by independent parties

3) Extensible analysis = analysis supporting independent variations

4) Study = Design + implementation + analysis

5) Replicable study = A study that, when executed by independent parties,
produces statistically compatible interpretations

## Open questions

1) What is an environment for an analysis?

2) What are independent parties? (See Kahneman's [concept of "replication etiquette"](https://www.scribd.com/document/225285909/Kahneman-Commentary))

3) What are independent variations on an analysis and why are they important?

4) When are two interpretations of related or identical studies statistically compatible?  Reformulate in terms of the standards of replicability for a given field.  Example: GWAS catalog concept of replicated finding (see, e.g., P. Kraft et al., Stat. Sci. 2009).

## GWAS: [p-values are not created equal](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2865141/)

```{r foo,echo=FALSE}
im = readPNG("images/KraftGWASreplication.png")
grid.raster(im,height=1.2)
```

## GWAS replication recommendations in Kraft et al. 2009

* Test the same marker 
* Use the same analytic methods; report
the final model in as much detail as possible so that other investigators
can judge the fit of that model in other datasets
* Try to use the same phenotype

* Issues: 
    * measuring heterogeneity of effects
    * choosing models and studies for metaanalysis
    * understanding failure to replicate

## STREGA: genetic association reporting guidelines

```{r foo2,echo=FALSE}
im = readPNG("images/StregaTitle.png")
grid.raster(im)
```

## Guideline 12
```{r foo3,echo=FALSE}
im = readPNG("images/strega12.png")
grid.raster(im)
```

## Guidelines 13-16
```{r foo4,echo=FALSE}
im = readPNG("images/strega13_16.png")
grid.raster(im)
```

## Upshots

- Replicability is a live concept for population genetics research
- Contributions to literature are managed through voluntary guidelines
- The EBI/NHGRI GWAS catalog includes a formal replication concept for every reported SNP-phenotype association

## GWAS catalog eligibility

```{r foo5,echo=FALSE}
im = readPNG("images/gwascat.png")
grid.raster(im)
```

## Genetic medical counseling errors

```{r foo6,echo=FALSE}
im = readPNG("images/exacNEJMtitle.png")
grid.raster(im)
```

## Headline

```{r foo7,echo=FALSE}
im = readPNG("images/exacNYTpic.png")
grid.raster(im)
```

## Consequences of error

```{r foo8,echo=FALSE}
im = readPNG("images/exacNYTtext.png")
grid.raster(im)
```

## Moving forward

```{r foo9,echo=FALSE}
im = readPNG("images/exacUseRecs.png")
grid.raster(im)
```

## A view of Exome Aggregation Consortium

```{r foo10,echo=FALSE}
im = readPNG("images/exacHistosPCA.png")
grid.raster(im)
```

## Upshots

- Genetic epidemiology's use of GWAS includes explicit requirement for replication
- Implementation of the requirement uses the concepts
    - *combined p-value* for base and replication studies
    - in absence of combined p-value, separate thresholds of $\log_{10} p < -5$ in two studies
- More care is needed in assessing evidence of genetic risk in medical applications
- Accurate interpretation of large-scale aggregation/stratification of genetic findings must become routine

## Some material from the NAS workshop: Benjamini

- Poor replicability of animal phenotyping studies is due to inaccurate reckoning of variability: genotype $\times$ laboratory interactions are present but contribution to variance is not known
- *Quantitative replicability* is present when **two or more** studies agree on a given finding
    - For two studies with null hypotheses $H_{01}$, $H_{02}$, replicabilityrequires that the union null $H_{01} \cup H_{02}$ is rejected in favor of the conjunction alternative
    - [$r$-value](https://arxiv.org/pdf/1502.00088) = $\max(P_1, P_2)$
    - generalizations to larger syntheses change the character of interpretation: not just summarizing evidence on existence of an effect, but measuring evidence of a **replicable** effect

## Some material from the NAS workshop: Boos

- Replication condition for a scalar summary index $T$ computed in two experiments: $(T_1, T_2) =_d (T_2, T_1)$
- $P(T_2 > T_1) = 1/2$, so if $T_1$ is "borderline significant" there is substantial probability that a genuinely replicating experiment will not be "significant"
- P-values possess statistical variability, yet are seldom reported with intervals or standard errors
- Replication probability for one-sample normal mean with $n$ *independent* samples
$$
RP = 1 - F_{t,n-1,ncp}(F^{-1}_{t,n-1}(1-\alpha))
$$

## Boos: Replication probabilities are low for conventional thresholds

```{r foo11,echo=FALSE}
im = readPNG("images/boosStefRP.png")
grid.raster(im,width=.95)
```

## Some material from the NAS workshop: Valen Johnson ([PNAS paper](http://www.pnas.org/content/110/48/19313.full.pdf))

```{r foo12,echo=FALSE}
im = readPNG("images/johnson1.png")
grid.raster(im,width=.99)
```

## Costs of more stringent thresholds: $N_{strong} > 1.7 \times N_{conventional}$

```{r foo13,echo=FALSE}
im = readPNG("images/johnson2.png")
grid.raster(im,width=.99)
```

## Upshots

- Statistical theory for enhancing replicability of analyses of independent experiments is substantial
- Costs of increasing replicability are non-trivial
- Preserving the reputation of advanced science may well justify the expenditure
- Cost-efficient experimental designs are not much discussed in NAS workshop

## Transition: Some case studies in reproducibility

- What I have done or promoted to simplify reproducible and extensible analyses in various domains
    - Cancer genomics: entering the Duke ovarian cancer fray
    - Bioconductor: continuous integration
    - Immune Tolerance Network "trialshare": bridging publication to data and interactive analysis
    - barca: excavating old code on request
